---
title: 'The Best of Both Worlds: Stochastic and Adversarial Bandits'
abstract: We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal)
  whose regret is (essentially) optimal both for adversarial rewards and for stochastic
  rewards. Specifically, SAO combines the \emphO(√\emphn) worst-case regret of Exp3
  (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a)
  for stochastic rewards. Adversarial rewards and stochastic rewards are the two main
  settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats
  them separately, and does not attempt to jointly optimize for both. This result
  falls into the general agenda to design algorithms that combine the optimal worst-case
  performance with improved guarantees for “nice” problem instances.
pdf: http://proceedings.mlr.press/v23/bubeck12b/bubeck12b.pdf
layout: inproceedings
id: bubeck12b
month: 0
firstpage: 42
lastpage: 42
page: 42-42
sections: 
author:
- given: SÃ©bastien
  family: Bubeck
- given: Aleksandrs
  family: Slivkins
date: 2012-06-16
address: Edinburgh, Scotland
publisher: PMLR
container-title: Proceedings of the 25th Annual Conference on Learning Theory
volume: '23'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 6
  - 16
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
