---
title: Private Convex Empirical Risk Minimization and High-dimensional Regression
abstract: 'We consider \emph{differentially private} algorithms for convex empirical
  risk minimization (ERM). Differential privacy (Dwork et al., 2006b) is a recently
  introduced notion of privacy which guarantees that an algorithm''s output does not
  depend on the data of any individual in the dataset. This is crucial in fields that
  handle sensitive data, such as genomics, collaborative filtering, and economics.
  Our motivation is the design of private algorithms for sparse learning problems,
  in which one aims to find solutions (e.g., regression parameters) with few non-zero
  coefficients. To this end: (a) We significantly extend the analysis of the ``objective
  perturbation'''' algorithm of Chaudhuri et al. (2011) for convex ERM problems. We
  show that their method can be modified to use less noise (be more accurate), and
  to apply to problems with hard constraints and non-differentiable regularizers.
  We also give a tighter, data-dependent analysis of the additional error introduced
  by their method. A key tool in our analysis is a new nontrivial limit theorem for
  differential privacy which is of independent interest: if a sequence of differentially
  private algorithms converges, in a \emph{weak} sense, then the limit algorithm is
  also differentially private. In particular, our methods give the best known algorithms
  for differentially private linear regression. These methods work in settings where
  the number of parameters p is less than the number of samples $n$. (b) We give the
  first two private algorithms for \emph{sparse} regression problems in high-dimensional
  settings, where $p$ is much larger than $n$. We analyze their performance for linear
  regression: under standard assumptions on the data, our algorithms have vanishing
  empirical risk for $n = poly(s, \log p)$ when there exists a good regression vector
  with s nonzero coefficients. Our algorithms demonstrate that randomized algorithms
  for sparse regression problems can be both stable and accurate - a combination which
  is impossible for deterministic algorithms.'
pdf: "./kifer12/kifer12.pdf"
layout: inproceedings
id: kifer12
month: 0
firstpage: 25
lastpage: 25
page: 25-25
origpdf: http://jmlr.org/proceedings/papers/v23/kifer12/kifer12.pdf
sections: 
author:
- given: Daniel
  family: Kifer
- given: Adam
  family: Smith
- given: Abhradeep
  family: Thakurta
date: '2012-06-16 00:00:25'
publisher: PMLR
---
