---
title: New Bounds for Learning Intervals with Implications for Semi-Supervised Learning
abstract: We study learning of initial intervals in the prediction model. We show
  that for each distribution \emphD over the domain, there is an algorithm \emphA_D,
  whose probability of a mistake in round m is at most \emph(½ + o(1))/m. We also
  show that the best possible bound that can be achieved in the case in which the
  same algorithm \emphA must be applied for all distributions \emphD is at least (^1⁄_√\emphe
  - o(1))^1⁄_\emphm > (^3⁄_5-o(1))^1⁄_\emphm. Informally, “knowing” the distribution
  \emphD enables an algorithm to reduce its error rate by a constant factor strictly
  greater than 1. As advocated by Ben-David et al. (2008), knowledge of \emphD can
  be viewed as an idealized proxy for a large number of unlabeled examples.
pdf: http://proceedings.mlr.press/v23/helmbold12/helmbold12.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: helmbold12
month: 0
tex_title: New Bounds for Learning Intervals with Implications for Semi-Supervised
  Learning
firstpage: '30.1'
lastpage: '30.15'
page: 30.1-30.15
order: 30
cycles: false
author:
- given: David P.
  family: Helmbold
- given: Philip M.
  family: Long
date: 2012-06-16
address: Edinburgh, Scotland
publisher: PMLR
container-title: Proceedings of the 25th Annual Conference on Learning Theory
volume: '23'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 6
  - 16
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
