---
title: New Bounds for Learning Intervals with Implications for Semi-Supervised Learning
abstract: We study learning of initial intervals in the prediction model. We show
  that for each distribution \emph{D} over the domain, there is an algorithm \emph{A_{D}},
  whose probability of a mistake in round m is at most \emph{(½ + o(1))/m}. We also
  show that the best possible bound that can be achieved in the case in which the
  same algorithm \emph{A} must be applied for all distributions \emph{D} is at least
  (^{1}⁄_{√\emph{e}} - o(1))^{1}⁄_{\emph{m}} > (^{3}⁄_{5}-o(1))^{1}⁄_{\emph{m}}. Informally,
  ``knowing'' the distribution \emph{D} enables an algorithm to reduce its error rate
  by a constant factor strictly greater than 1. As advocated by Ben-David et al. (2008),
  knowledge of \emph{D} can be viewed as an idealized proxy for a large number of
  unlabeled examples.
pdf: "./helmbold12/helmbold12.pdf"
layout: inproceedings
id: helmbold12
month: 0
firstpage: 30
lastpage: 30
page: 30-30
origpdf: http://jmlr.org/proceedings/papers/v23/helmbold12/helmbold12.pdf
sections: 
author:
- given: David P.
  family: Helmbold
- given: Philip M.
  family: Long
date: '2012-06-16 00:00:30'
publisher: PMLR
---
