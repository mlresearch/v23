---
title: 'Open Problem: Is Averaging Needed for Strongly Convex Stochastic Gradient
  Descent?'
abstract: Stochastic gradient descent (SGD) is a simple and very popular iterative
  method to solve stochastic optimization problems which arise in machine learning.
  A common practice is to return the average of the SGD iterates. While the utility
  of this is well-understood for general convex problems, the situation is much less
  clear for strongly convex problems (such as solving SVM). Although the standard
  analysis in the strongly convex case requires averaging, it was recently shown that
  this actually degrades the convergence rate, and a better rate is obtainable by
  averaging just a suffix of the iterates. The question we pose is whether averaging
  is needed at all to get optimal rates.
pdf: http://jmlr.org/proceedings/papers/v23/shamir12/shamir12.pdf
layout: inproceedings
id: shamir12
month: 0
firstpage: 47
lastpage: 47
page: 47-47
sections: 
author:
- given: Ohad
  family: Shamir
reponame: v23
date: 2012-06-16
address: Edinburgh, Scotland
publisher: PMLR
container-title: Proceedings of the 25th Annual Conference on Learning Theory
volume: '23'
genre: inproceedings
issued:
  date-parts:
  - 2012
  - 6
  - 16
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
