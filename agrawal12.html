<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

<link rel="alternate" type="application/rss+xml" href="http://www.jmlr.org/jmlr.xml" title="JMLR RSS">
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
<style type="text/css">
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style>
<script language="JavaScript"> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</script>


<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>

<div id="content">
<h2>Analysis of Thompson Sampling for the Multi-armed Bandit Problem</h2>
<p><b><i>Shipra
  Agrawal and Navin Goyal</i></b><i> </i>JMLR W&amp;CP 23: 39.1 - 39.26, 2012</p>
<h3>Abstract</h3>
<p>The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off
in sequential decision problems. Many algorithms are now available for this well-studied problem.
One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm,
referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose
an arm to play according to its probability of being the best arm. Thompson Sampling algorithm
has experimentally been shown to be close to optimal. In addition, it is efficient to implement and
exhibits several desirable properties such as small regret for delayed feedback. However, theoretical
understanding of this algorithm was quite limited. In this paper, for the first time, we show that
Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed
bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret
in time <i>T</i> is <i>O</i>(<sup>(ln <i>T</i>)</sup> &frasl; <sub>&Delta;</sub> + <sup>1</sup> &frasl; <sub>&Delta;<sup>3</sup></sub>). And, for the stochastic <i>N</i>-armed bandit problem, the expected regret in
time <i>T</i> is <i>O</i>([&Sigma;<sub>i=2..N</sub> <sup>1</sup> &frasl; <sub>(&Delta;<sub><i>i</i></sub>)<sup>2</sup></sub>] ln <i>T</i>). Our bounds are optimal but for the dependence on &Delta;<sub><i>i</i></sub> and the
constant factors in big-Oh.</p>
</div>
 <div id="fixed">
<br>
<a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.csail.mit.edu/jmlr.jpg" align="right" border="0"></a>
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.csail.mit.edu/RSS.gif" class="rss" alt="RSS Feed">
</a>



</p></div>
 
<p></p><center>Page last modified on Sat June 16 2012 22:30 2012.</center>

<p> 

<table width="100%"> <tbody><tr>
<td align="right"><font size="-1">Copyright 
@ <a target="_top" href="http://www.jmlr.org/">JMLR</a> 2012.  All rights
reserved.</font></td> </tr> </tbody></table>
</p></body></html>